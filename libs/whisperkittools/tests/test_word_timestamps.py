#
# For licensing see accompanying LICENSE.md file.
# Copyright (C) 2024 Argmax, Inc. All Rights Reserved.
#

import json
import os
import unittest

from argmaxtools.utils import get_logger
from huggingface_hub import hf_hub_download

from whisperkit._constants import TEST_DATA_REPO
from whisperkit.evaluate.normalize_en import EnglishTextNormalizer
from whisperkit.pipelines import WhisperKit

logger = get_logger(__name__)

text_normalizer = EnglishTextNormalizer()

ERROR_MARGIN = 0.25  # 250 ms
PERCENT_WORDS_WITHIN_ERROR_MARGIN = 0.8
TEST_FIRST_N_WORDS = 60


class TestWordTimestamps(unittest.TestCase):
    """ Test the accuracy of word-level timestamps generated by the WhisperKit pipeline
    against the OpenAI API reference result
    """
    @classmethod
    def setUpClass(cls):
        cls.pipeline = WhisperKit(whisper_version="openai/whisper-tiny.en", out_dir="external")
        cls.pipeline._word_timestamps = True

        with open(hf_hub_download(
            repo_id=TEST_DATA_REPO,
            filename="ted_60_openai_api_2_27_2024.json",
            repo_type="dataset"
        ), "r") as f:
            openai_api_result = json.load(f)

        whisperkit_result = cls.pipeline(hf_hub_download(
            repo_id=TEST_DATA_REPO,
            filename="ted_60.wav",
            repo_type="dataset"
        ))

        # FIXME (atiorh): A similar rule should be upstreamed to WhisperKit
        cls.test_result = [
            word for segment in whisperkit_result["segments"] for word in segment["words"]
            if not (word["end"] != word["start"] and word["end"] > 29)
        ]
        [word.pop("probability") for word in cls.test_result]
        [word.pop("tokens") for word in cls.test_result]
        cls.reference_result = openai_api_result["words"]

    def test_accuracy(self):
        pass_rate = 0
        for idx, (word_test, word_reference) in enumerate(zip(self.test_result[:TEST_FIRST_N_WORDS],
                                                              self.reference_result[:TEST_FIRST_N_WORDS])):
            logger.info(f"Word #{idx}: Test={word_test}, Reference: {word_reference}")
            self.assertEqual(
                text_normalizer(word_test["word"]),
                text_normalizer(word_reference["word"])
            )
            if abs(word_test["start"] - word_reference["start"]) < ERROR_MARGIN and \
                    abs(word_test["end"] - word_reference["end"]) < ERROR_MARGIN:
                pass_rate += 1
                logger.info("Match")
            else:
                logger.info("Mismatch")

        pass_rate /= min(len(self.test_result), TEST_FIRST_N_WORDS)
        logger.info(f"Pass rate: {pass_rate*100:.2f}% of words within {ERROR_MARGIN * 1000} ms error margin")

        self.assertGreaterEqual(pass_rate, PERCENT_WORDS_WITHIN_ERROR_MARGIN)


if __name__ == "__main__":
    suite = unittest.TestSuite()
    suite.addTest(TestWordTimestamps("test_accuracy"))

    if os.getenv("DEBUG", False):
        suite.debug()
    else:
        runner = unittest.TextTestRunner()
        runner.run(suite)
